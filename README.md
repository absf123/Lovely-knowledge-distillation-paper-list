# Lovely-knowledge-distillation-paper-list
knowledge distillation paper list (focusing graph neural network)


## Contents

- [Knowledge distillation types](#knowledgedistillationtypes)
  - [Individual KD : Conventional KD](#ikd) 
    - [logits](#logits)
    - [feature map](#featuremap)
  - [Relational KD](#rkd)
- [Multi teacher](#multiteacher)
- [Self KD](#selfkd)
- [Graph KD](#graphkd)
- [2021 : Major conference paper](#mcp)
<a name="knowledgedistillationtypes" />

## Knowledge distillation types

<a name="multiteacher" />

## Multi teacher

<a name="selfkd" />

## Self KD

<a name="graphkd" />

## Graph KD
1. Distilling Knowledge from Graph Convolutional Networks, 2020, CVPR [[paper]][1.1] [[code]][1.2]
2. Amalgamating Knowledge from Heterogeneous Graph Neural Networks, 2021, CVPR [[paper]][2.1] [[code]][2.2]

<a name="mcp" />

## 2021 : Major conference paper



[1.1]: https://arxiv.org/abs/2003.10477
[1.2]: https://github.com/ihollywhy/DistillGCN.PyTorch
[2.1]: https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Amalgamating_Knowledge_From_Heterogeneous_Graph_Neural_Networks_CVPR_2021_paper.pdf
[2.2]: https://github.com/ycjing/
